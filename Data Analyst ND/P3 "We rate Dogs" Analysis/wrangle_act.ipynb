{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs Twitter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#question\">Question</a></li>    \n",
    "<li><a href=\"#gathering\">Data Gathering</a></li>\n",
    "<li><a href=\"#assessing\">Data Assessment</a></li>\n",
    "<li><a href=\"#cleaning\">Data Cleaning</a></li>\n",
    "<li><a href=\"#analyzevisualize\">Data Analysis and Visualisations</a></li>\n",
    "<li><a href=\"#report\">Final Report</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to wrangle WeRateDogs Twitter data to create interesting and trustworthy analysis and visualizations. \n",
    "\n",
    "Wrangling activities include:\n",
    "* Gathering data from 3 different sources (abbreviation used DF1, DF2, DF3).\n",
    "* Assessing data to identify quality and tidiness issues. \n",
    "* Cleaning data, that includes activities: definition, coding, testing. \n",
    "* Storing, analysing, and visualising interesting insights.\n",
    "* Reporting on wrangling and data analysis efforts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question'></a>\n",
    "## Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not a Twitter user nor a fan, therefore I do not have a \"strong\" relationship with this dataset, however I came up with the following questions that I'd like to investigate:\n",
    "* **What is the success rate for image prediction?**\n",
    "* **What is the growth rate for WeRateDogs user ( by counting the followers)?**\n",
    "* **Is there a correlation between twiting and month (i.e. is twitting affected by the season)?**\n",
    "* **Which tweet characteristics predict high retweeting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages and set plots to be embedded inline\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from timeit import default_timer as timer\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF1 - Load Image prediction data (tsv format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file is hosted on Udacity's servers and is downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "Dictionary of the attributes:\n",
    "* p1 is the algorithm's #1 prediction for the image in the tweet → golden retriever\n",
    "* p1_conf is how confident the algorithm is in its #1 prediction → 95%\n",
    "* p1_dog is whether or not the #1 prediction is a breed of dog → TRUE\n",
    "* p2 is the algorithm's second most likely prediction → Labrador retriever\n",
    "* p2_conf is how confident the algorithm is in its #2 prediction → 1%\n",
    "* p2_dog is whether or not the #2 prediction is a breed of dog → TRUE\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder if it's not craeted\n",
    "folder_name = 'Dataset'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs('Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save link\n",
    "url='https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the content from the saved link\n",
    "with open(os.path.join(folder_name, url.split(\"/\")[-1]), mode = 'wb') as file:\n",
    "    file.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Dataset repository\n",
    "os.listdir(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Image Prediction into Pandas DataFrame\n",
    "df1=pd.read_csv('Dataset/image-predictions.tsv', sep='\\t')\n",
    "df1.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF2 - Load Twitter Archive data (csv format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WeRateDogs Twitter archive data file is given, therefore I'm downloading file manually and uploading into the pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('Dataset/twitter-archive-enhanced.csv')\n",
    "df2.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF3 - Load Twitter API data (json format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tweet IDs in the WeRateDogs Twitter archive, I'm query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file. Each tweet's JSON data should be written to its own line. Then read this .txt file line by line into a pandas DataFrame with (at minimum) tweet ID, retweet count, and favorite count. \n",
    "\n",
    "Additional information: [pandas API reference](https://pandas.pydata.org/pandas-docs/stable/api.html) for detailed usage information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query via API and save respond to txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Query Twitter API for each tweet in the Twitter archive and save JSON in a text file\n",
    "# These are hidden to comply with Twitter's API terms and conditions\n",
    "consumer_key = 'HIDDEN'\n",
    "consumer_secret = 'HIDDEN'\n",
    "access_token = 'HIDDEN'\n",
    "access_secret = 'HIDDEN'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "tweet_ids = df2.tweet_id.values\n",
    "len(tweet_ids)\n",
    "\n",
    "# Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n",
    "count = 0\n",
    "fails_dict = {}\n",
    "start = timer()\n",
    "# Save each tweet's returned JSON as a new line in a .txt file\n",
    "with open('Dataset/tweet_json.txt', 'w') as outfile:\n",
    "    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n",
    "    for tweet_id in tweet_ids:\n",
    "        count += 1\n",
    "        print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            print(\"Success\")\n",
    "            json.dump(tweet_json, outfile)\n",
    "            outfile.write('\\n')\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Fail\")\n",
    "            fails_dict[tweet_id] = e\n",
    "            pass\n",
    "end = timer()\n",
    "print(end - start)\n",
    "print(fails_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read API data line by line\n",
    "\n",
    "Here we read the txt file line by line and store values into a list object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read line by line and save into a list\n",
    "count=0\n",
    "tweets_data = []\n",
    "inputfile=open('Dataset/tweet_json.txt', \"r\")\n",
    "for line in inputfile:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except: \n",
    "        count+=1\n",
    "        continue\n",
    "print('Failed:{}'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check json structure\n",
    "Pandas DataFrame function allows me to load the list object and to have a quick overview of the data structure and data types. \n",
    "However this data load approach does not comply with the data tidiness requirements, that are :\n",
    "* Each variable forms a column\n",
    "* Each observation forms a row\n",
    "* Each observational unit forms a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review=pd.DataFrame(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have an overview of data structure and data types\n",
    "review.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An overview of a specific variable\n",
    "review.entities[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the attributes and save into DF3\n",
    "\n",
    "Here I'm specifying the attributes of my interest and loading into pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Had to add a dummy dictionary for the empty list, so that I could grab Hashtags when it has a value.\n",
    "for tweet in tweets_data:\n",
    "    if tweet['entities']['hashtags'] == []:\n",
    "        tweet['entities']['hashtags'] = [{'text': '', 'indices':[]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the specified attributes and load into pandas DataFrame\n",
    "d = []\n",
    "for tweet in tweets_data:\n",
    "    d.append({\n",
    "        'tweet_id': tweet['id'],\n",
    "        'tweet_favourite_count' : tweet['favorite_count'],\n",
    "        'tweet_hashtags' : tweet['entities']['hashtags'][0]['text'],\n",
    "        'retweet_count' : tweet['retweet_count'],\n",
    "        'retweeted' : tweet['retweeted'],\n",
    "        'user_id': tweet['user']['id'],\n",
    "        'user_followers_count': tweet['user']['followers_count'],\n",
    "        'user_friends_count': tweet['user']['friends_count'],\n",
    "        'user_listed_count': tweet['user']['listed_count'],\n",
    "        'user_favourites_count': tweet['user']['favourites_count'],\n",
    "        'user_statuses_count': tweet['user']['statuses_count'],\n",
    "    })\n",
    "\n",
    "df3=pd.DataFrame(d)\n",
    "df3.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering Summary \n",
    "> * All 3 data sources are loaded\n",
    "> * DataFrames are named accorfingly DF1, DF2, DF3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessing'></a>\n",
    "## Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After gathering each of the above pieces of data, requires assessment that is done visually and programmatically. \n",
    "There are two types of unclean data concepts:\n",
    "* **Dirty data**, also known as **low quality data**. Low quality data has **content issues**.\n",
    "* **Messy data**, also known as **untidy data**. Untidy data has **structural issues**.\n",
    "\n",
    "Identified quality and tidiness issues are documented in the summary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF1 - Image prediction DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assessing data visually\n",
    "df1.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing data programatically\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting unique values\n",
    "df1.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking quantitative information\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary for DF1 Assessing :\n",
    "> **Overview:**\n",
    " * At least 75% of the records have only 1 image. \n",
    " * P1 confidence level is very high. \n",
    "\n",
    "> **Quality issues:** \n",
    "* It could be interested to analyse against the Image format data which is currently imbedded in the image link.\n",
    "\n",
    ">**Tidiness issues:** \n",
    "* Prediction columns repeating per each prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF2 - Twitter Archive DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing data visually\n",
    "df2.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing data programatically\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting unique values\n",
    "df2.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing source categories\n",
    "df2.source.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking quantitative information\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary for DF2 Assessing : \n",
    "> **Overview:**\n",
    " * There are 2356 tweets and tweet_id is unique identificator.\n",
    " * There are 4 source categories. \n",
    " * rating_numerator and rating_denominator values have outliers.\n",
    "\n",
    "> **Quality issues:** \n",
    "* Source information is presented by a link. \n",
    "* It was mentioned that rating_numerator and rating_denominator values might have arrors.\n",
    "* It was mentioned that Dogtionary attributes might have have errors. \n",
    "* Since I'm interested in have at leats one plot with the timeline I need to have a user friendly date format.\n",
    "\n",
    ">**Tidiness issues:** \n",
    "* Dogtionary types are listed in the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF3 - Twitter API DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing data visually\n",
    "df3.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing data programatically\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting unique values\n",
    "df3.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking quantitative information\n",
    "df3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary for DF3 Assessing :\n",
    "> **Quality issues:** \n",
    "* Retweet value is always False, which is wrong.  \n",
    "* user_friends_count and user_statuses_count have always the same value, therefore I'm not interested in this columns.\n",
    "\n",
    ">**Tidiness issues:** \n",
    "* User information should be separated from the Twitter data, since the user is always the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column duplicates within all 3 DataFrames\n",
    "all_columns = pd.Series(list(df1) + list(df2) + list(df3))\n",
    "all_columns[all_columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data cleaning activities are performed that includes: \n",
    "* Define\n",
    "* Code\n",
    "* Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a working copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c = df1.copy() # For the Image prediction\n",
    "df2_c = df2.copy() # For the Archived Twitter\n",
    "df3_c = df3.copy() # For the Twitter API data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Twitter Archive and Twitter API Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Join Twitter DataFrames ( Tidiness )\n",
    "\n",
    "Join both DataFrames on the attribute twitter_id. Twitter id is a unique record identifaer. I'm using inner join type because I'm interested only in the records that are in both DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code, i.e. join the DataFrames\n",
    "t_merge = df2_c.merge(df3_c, left_on='tweet_id', right_on='tweet_id')\n",
    "t_merge = pd.DataFrame(t_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test visually\n",
    "t_merge.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test by counting the records\n",
    "print('Count of records in Twitter Archive is : {}'.format(df2_c['tweet_id'].count()))\n",
    "print('Count of records in Twitter API is : {}'.format(df3_c['tweet_id'].count()))\n",
    "print('Count of records in the joined file is : {}'.format(t_merge['tweet_id'].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a new attribute to indicate twitter Year and Month ( Quality )\n",
    "I have intention to use timeline information in the visualisation, therefore I need to create a date format that is more user friendly. The lowest granularity I'm interested is month. \n",
    "\n",
    "By using a regular expression, I'm going to extract year and month information with the extract function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. Extract year and month information\n",
    "Timestamp = t_merge['timestamp'].str.extract('(\\d+)-(\\d+)').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. Concatinate year and month values with separator -\n",
    "t_merge['year_month'] = Timestamp[0] + \"-\" + Timestamp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "t_merge['year_month'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Image Prediction File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Add image format ( Quality )\n",
    "\n",
    "From the jpg_url I want to extract the image format and save it into a separate column. By using a regular expression, I'm going to split the string and grab the last value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A code to extract image format\n",
    "df1_c['im_type'] = df1_c['jpg_url'].str.split(\".\").str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique image formats\n",
    "df1_c['im_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Transform the columns ( Tidiness )\n",
    "Prediction columns as: p(x), p(x)_conf, p(x)_dog; should be merged and a new column with prediction algorithm No. should be added. \n",
    "\n",
    "\n",
    "First, I am creating 3 separate dataframes with new column prd and defaulted values p1, p2, or p3. After, it I'm concatinating into a single DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_c.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for prediction 1\n",
    "prd1 = pd.DataFrame(df1_c, columns=['tweet_id','img_num', 'im_type', 'p1', 'p1_conf', 'p1_dog'])\n",
    "prd1['prd']='p1'\n",
    "prd1 = prd1.rename(columns={'tweet_id':'tweet_id','img_num':'img_num', 'im_type':'im_type', \n",
    "                            'prd':'prd', 'p1':'guess', 'p1_conf':'conf', 'p1_dog':'dog'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test visually\n",
    "prd1.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for prediction 2\n",
    "prd2 = pd.DataFrame(df1_c, columns=['tweet_id','img_num', 'im_type', 'p2', 'p2_conf', 'p2_dog'])\n",
    "prd2['prd']='p2'\n",
    "prd2 = prd2.rename(columns={'tweet_id':'tweet_id','img_num':'img_num', 'im_type':'im_type', \n",
    "                            'prd':'prd', 'p2':'guess', 'p2_conf':'conf', 'p2_dog':'dog'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test visually\n",
    "prd2.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for prediction 3\n",
    "prd3 = pd.DataFrame(df1_c, columns=['tweet_id','img_num', 'im_type', 'p3', 'p3_conf', 'p3_dog'])\n",
    "prd3['prd']='p3'\n",
    "prd3 = prd3.rename(columns={'tweet_id':'tweet_id','img_num':'img_num', 'im_type':'im_type', \n",
    "                            'prd':'prd', 'p3':'guess', 'p3_conf':'conf', 'p3_dog':'dog'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test visually\n",
    "prd3.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinate all 3 DataFrames with a new attribute \"prd\" indicating whether it's the prediction 1, 2, or 3\n",
    "frames = [prd1, prd2, prd3]\n",
    "prediction_clean=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results\n",
    "prediction_clean.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Save the clean file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder if it's not craeted\n",
    "folder_name = 'Clean_dataset'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs('Clean_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a csv format\n",
    "df.to_csv(prediction_clean, 'Clean_dataset/prediction_clean.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Twitter User DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a new DataFrame ( Tidiness & Quality )\n",
    "I'll grab interesting user attributes and save it into a separate dataframe. User is the same for all the twitters, but I'm more interested in the growth of followers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the attributes\n",
    "t_merge.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new User Dataframe with user information and twitter dates\n",
    "user=pd.DataFrame(t_merge, columns=['timestamp', 'year_month', 'user_favourites_count', 'user_followers_count', \n",
    "                                    'user_id', 'user_listed_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test visually\n",
    "user.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicated records\n",
    "user.duplicated(['user_favourites_count', 'user_followers_count', \n",
    "                 'user_id', 'user_listed_count']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates and leave only unique values\n",
    "user.drop_duplicates(['user_favourites_count', 'user_followers_count', \n",
    "                      'user_id', 'user_listed_count', 'year_month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique user records\n",
    "user['user_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test visually\n",
    "user.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the insights\n",
    "print ('The followers count started with {}'.format(user['user_followers_count'].min()))\n",
    "print ('The followers count ended with {}'.format(user['user_followers_count'].max()))\n",
    "print ('The follower count within {} and {} time period increase by {}%'.format \n",
    "       ((user['year_month'].min()), (user['year_month'].max()), round((user['user_followers_count'].max())/(user['user_followers_count'].min())),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Save cleaned User.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a csv format\n",
    "df.to_csv(user, 'Clean_dataset/user_clean.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Twitter information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Clean dataframe by removing unecessary attributes ( Quality )\n",
    "I want to reduce the list of attributes and keep only the ones I'm going to clean or use for visualisations. All the other attributes is droped from this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the attributes and identify the ones to be dropped\n",
    "t_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop attribute, that I'm not going to use for the analysis\n",
    "t_merge.drop(columns=['user_favourites_count', 'user_followers_count', 'user_friends_count', 'user_id', \n",
    "                      'user_listed_count', 'user_statuses_count', 'in_reply_to_status_id',\n",
    "                      'in_reply_to_user_id', 'expanded_urls', 'retweeted_status_id', \n",
    "                      'retweeted_status_user_id'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test programatically \n",
    "t_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test visually\n",
    "t_merge.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extract source name from the link ( Quality )\n",
    "There are 4 types of sources that are presented by a link. I want to use regular expressions to extract the titles from a link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the source values\n",
    "t_merge.source.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code, i.e. split the spring and grab the source name\n",
    "t_merge['source'] = t_merge['source'].str.split(\">\").str[1].str.split(\"<\").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "t_merge['source'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Clean rating information ( Quality )\n",
    "\n",
    "As it was described in the description \"The **ratings** probably aren't all correct.\" Rating information is part of the text with the format \"digit/digit\". here are some examples of the text with the rating in the end, however as you see the same pattern might appear twice in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Text examples: \\n * {} or \\n * {} or \\n * {}'.format(t_merge['text'][2332], t_merge['text'][2333], t_merge['text'][2334]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every record has a text and I assume that every text, has rating information inside.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all the tweets have a text\n",
    "print('Count of records with empty text is {}'.format (t_merge['text'].isnull().sum()))\n",
    "# Check if there are empty ratings\n",
    "print('Count of records with empty rating is {}/{}'.format ((t_merge['rating_numerator'].isnull().sum()), (t_merge['rating_denominator'].isnull().sum())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to grab the last \"digit/digit\" pattern in the string, I need to reverse the string, but before this I'll get rid of the link information that is in the end of string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. Remove the link and reverse the string. \n",
    "rating = t_merge['text'].str.split('http').str[0].str[::-1]\n",
    "# Grap the first pattern \n",
    "rating = rating.str.extract('(\\d+)/(\\d+)')\n",
    "# Store nto the rating format and do a back reverse\n",
    "full_rating = (rating[0]+'/'+rating[1]).str[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test by comparing the ratings\n",
    "print('Discrepencies with numerator values : {}'.format((t_merge[t_merge['rating_numerator'] != full_rating.str.split('/').str[0].astype(int)]['tweet_id'].count())))\n",
    "print('Discrepencies with denominator values : {}'.format( (t_merge[t_merge['rating_denominator'] != full_rating.str.split('/').str[1].astype(int)]['tweet_id'].count())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite with the new values\n",
    "t_merge['rating_numerator']=full_rating.str.split('/').str[0].astype(int)\n",
    "t_merge['rating_denominator']=full_rating.str.split('/').str[1].astype(int)\n",
    "t_merge.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Clean Dogtionary information ( Quality )\n",
    "\n",
    "As it was described in the description \"The ratings probably aren't all correct. Same goes for the dog names and **probably dog stages**\".\n",
    "\n",
    "I'm going to scan the text and search if the categories were mentioned in the text, then I store the result in the new columns and do a comparison with the old values. Eventually old values is replace by the new ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. Scan the text and fill in a boolean if it was mentioned in the text\n",
    "t_merge['doggo1'] = t_merge['text'].str.contains('doggo')\n",
    "t_merge['floofer1'] = t_merge['text'].str.contains('floofer')\n",
    "t_merge['pupper1'] = t_merge['text'].str.contains('pupper')\n",
    "t_merge['puppo1'] = t_merge['text'].str.contains('puppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. Correct value from bollean to the standard ones so that I could do a comparison\n",
    "t_merge['doggo1'] = t_merge['doggo1'].map({True:'doggo',False:'None'})\n",
    "t_merge['floofer1'] = t_merge['floofer1'].map({True:'floofer',False:'None'})\n",
    "t_merge['pupper1'] = t_merge['pupper1'].map({True:'pupper',False:'None'})\n",
    "t_merge['puppo1'] = t_merge['puppo1'].map({True:'puppo',False:'None'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test by comparing old and new values\n",
    "print ('Mismatch for doggo : {}'.format(t_merge[t_merge['doggo1'] != t_merge['doggo']]['tweet_id'].count()))\n",
    "print ('Mismatch for floofer : {}'.format(t_merge[t_merge['floofer1'] != t_merge['floofer']]['tweet_id'].count()))\n",
    "print ('Mismatch for pupper : {}'.format(t_merge[t_merge['pupper1'] != t_merge['pupper']]['tweet_id'].count()))\n",
    "print ('Mismatch for puppo : {}'.format(t_merge[t_merge['puppo1'] != t_merge['puppo']]['tweet_id'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old values are replaced by new ones\n",
    "t_merge.drop(columns=['doggo','floofer','pupper','puppo'], inplace=True)\n",
    "t_merge = t_merge.rename(columns={'doggo1':'doggo','floofer1':'floofer', 'pupper1':'pupper', 'puppo1':'puppo'})\n",
    "t_merge.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Concatinate Dogtionary information into one column ( Tidiness )\n",
    "\n",
    "I'm going to merge Dogtionary category columns into one column by concatinating the strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. Concatinate strings\n",
    "t_merge.replace({'doggo':'None','floofer':'None','pupper':'None','puppo':'None'}, \"\", inplace=True)\n",
    "t_merge['dogtionary'] = t_merge['doggo'] + \" \" + t_merge['floofer'] + \" \" + t_merge['pupper'] + \" \" + t_merge['puppo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result\n",
    "t_merge['dogtionary'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove additional spaces and assign np.nan in case of null\n",
    "t_merge['dogtionary'] = t_merge['dogtionary'].astype(str)\n",
    "t_merge['dogtionary'] = t_merge['dogtionary'].str.rstrip(' ')\n",
    "t_merge['dogtionary'] = t_merge['dogtionary'].str.lstrip(' ')\n",
    "\n",
    "t_merge['dogtionary'] = t_merge['dogtionary'].str.replace (' ',' ')\n",
    "t_merge['dogtionary'] = t_merge['dogtionary'].str.replace ('  ',' ')\n",
    "t_merge['dogtionary'] = t_merge['dogtionary'].str.replace ('   ',' ')\n",
    "\n",
    "t_merge.loc[t_merge['dogtionary'] == '', 'dogtionary'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results\n",
    "t_merge['dogtionary'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns and test visually\n",
    "t_merge.drop(columns = ['doggo', 'floofer', 'pupper', 'puppo'], inplace=True)\n",
    "t_merge.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Correct Retweeted value ( Quality )\n",
    "Retweet information is always False which is incorrect. I'm going to add a logic, if retweet timestamp is not null, then rating should be True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values\n",
    "t_merge['retweeted'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to replace by True when the timestamp is not na\n",
    "t_merge['retweeted'] = pd.notna(t_merge['retweeted_status_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test programatically\n",
    "print('Count of retweeted_status_timestamp : {}'.format(t_merge['retweeted_status_timestamp'].count()))\n",
    "print('Count of retweeted status : {}'.format(t_merge['retweeted'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Save into a csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(t_merge, 'Clean_dataset/twitter_archive_master.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Data Cliening\n",
    "> All cleaned files can be found in 'Clean_dataset' repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('Clean_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analyzevisualize'></a>\n",
    "## Data Analysis, and Visualisations\n",
    "\n",
    "Here I come back to the questions that were raised in the beggining of the work and I'm going to visualise the answers for those questions:\n",
    "* What is the success rate for image prediction?\n",
    "* What is the growth rate for WeRateDogs user ( by counting the followers)?\n",
    "* Is there a correlation between twiting and month (i.e. is twitting affected by the season)?\n",
    "* Which tweet characteristics predict high retweeting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is the success rate for image prediction?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize = [20, 15]) \n",
    "plt.subplot(2, 2, 1) \n",
    "sb.countplot(data = prediction_clean, x = 'prd', hue = 'dog')\n",
    "plt.title ('Terms over Years');\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = np.arange(0, prediction_clean['conf'].max()+0.1, 0.1)\n",
    "prediction_clean.query('prd==\"p1\"')['conf'].hist(label='Prediction 1', color='g',alpha=0.5, bins=30);\n",
    "prediction_clean.query('prd==\"p2\"')['conf'].hist(label=\"Prediction 2\", color='r',alpha=0.5, bins=30);\n",
    "prediction_clean.query('prd==\"p3\"')['conf'].hist(label=\"Prediction 3\", color='b',alpha=0.5, bins=30);\n",
    "plt.legend()\n",
    "plt.title('StatedMonthlyIncome')\n",
    "plt.xlabel('StatedMonthlyIncome');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the growth rate for WeRateDogs user ( by counting the followers)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a correlation between twiting and month (i.e. is twitting affected by the season)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which tweet characteristics predict high retweeting?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "plt.figure(figsize = [15, 5]) \n",
    "\n",
    "plt.subplot(1, 2, 1) \n",
    "sorted_counts = df['IsBorrowerHomeowner'].value_counts()\n",
    "plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,\n",
    "        counterclock = False, wedgeprops = {'width' : 0.4});\n",
    "plt.title('Is Borrower Homeowner?');\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sorted_counts = df['IncomeVerifiable'].value_counts()\n",
    "plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,\n",
    "        counterclock = False, wedgeprops = {'width' : 0.4});\n",
    "plt.title('Income Verifiable?');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='report'></a>\n",
    "## Final Report\n",
    "Create a 300-600 word written report called wrangle_report.pdf or wrangle_report.html that briefly describes your wrangling efforts. This is to be framed as an internal document.\n",
    "\n",
    "Create a 250-word-minimum written report called act_report.pdf or act_report.html that communicates the insights and displays the visualization(s) produced from your wrangled data. This is to be framed as an external document, like a blog post or magazine article, for example.\n",
    "\n",
    "Both of these documents can be created in separate Jupyter Notebooks using the Markdown functionality of Jupyter Notebooks, then downloading those notebooks as PDF files or HTML files (see image below). You might prefer to use a word processor like Google Docs or Microsoft Word, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
